-- =============================================
-- REALISTIC SAMPLE RECORD FROM topic_embeddings
-- =============================================

-- When you query: SELECT * FROM topic_embeddings WHERE topic_id = 3;
-- You might get a record that looks like this:

/*
+----+----------+------------------+-------------------------+----------------------------+
| id | topic_id | embedding        | embedding_model         | source_text                |
+----+----------+------------------+-------------------------+----------------------------+
| 5  | 3        | [0.0123456789,   | text-embedding-ada-002  | Two Pointers: A programming|
|    |          |  -0.0234567891,  |                         | technique where you use two|
|    |          |  0.0345678912,   |                         | pointers to traverse an    |
|    |          |  -0.0456789123,  |                         | array or string, typically |
|    |          |  0.0567891234,   |                         | starting from different    |
|    |          |  ... (1531 more  |                         | positions. Common patterns |
|    |          |  dimensions)]    |                         | include: 1) One pointer at |
|    |          |                  |                         | start, one at end moving   |
|    |          |                  |                         | towards each other 2) Both |
|    |          |                  |                         | pointers starting at       |
|    |          |                  |                         | beginning, moving at       |
|    |          |                  |                         | different speeds 3) Sliding|
|    |          |                  |                         | window pattern. Used for:  |
|    |          |                  |                         | target sum problems,       |
|    |          |                  |                         | palindrome checking, cycle |
|    |          |                  |                         | detection. Prerequisites:  |
|    |          |                  |                         | Arrays, iteration patterns.|
+----+----------+------------------+-------------------------+----------------------------+
| created_at: 2024-01-15 14:30:22.123456                                                    |
+--------------------------------------------------------------------------------------------+
*/

-- =============================================
-- ACTUAL SAMPLE DATA YOU CAN INSERT FOR TESTING
-- =============================================

-- Sample embedding for "Two Pointers" topic (truncated to first 20 dimensions for readability)
INSERT INTO topic_embeddings (
    topic_id,
    embedding,
    embedding_model,
    source_text,
    created_at
) VALUES (
    3, -- Assuming "Two Pointers" has topic_id = 3
    
    -- Realistic embedding values (first 20 of 1536 dimensions)
    -- Note: Real embeddings from OpenAI would have all 1536 dimensions
    '[0.0123456789, -0.0234567891, 0.0345678912, -0.0456789123, 0.0567891234,
      -0.0678912345, 0.0789123456, -0.0891234567, 0.0912345678, -0.1023456789,
      0.1134567891, -0.1245678912, 0.1356789123, -0.1467891234, 0.1578912345,
      -0.1689123456, 0.1789234567, -0.1891234567, 0.1912345678, -0.2023456789]'::vector,
    
    'text-embedding-ada-002',
    
    'Two Pointers: A programming technique where you use two pointers to traverse an array or string, typically starting from different positions. Common patterns include: 1) One pointer at start, one at end, moving towards each other (useful for palindrome checking, target sum problems) 2) Both pointers starting at beginning, moving at different speeds (fast/slow pointer for cycle detection) 3) Sliding window where pointers maintain a window of elements. Key applications: finding pairs with target sum, removing duplicates, palindrome verification, cycle detection in linked lists, container with most water problems. Prerequisites: Arrays, basic iteration, understanding of indices. Related concepts: Sliding Window, Binary Search, Array traversal, Fast-Slow Pointers.',
    
    '2024-01-15 14:30:22.123456'
);

-- =============================================
-- WHAT EACH FIELD CONTAINS IN PRACTICE
-- =============================================

-- id: 5
-- Simple auto-incrementing primary key

-- topic_id: 3  
-- References the "Two Pointers" topic in your topics table

-- embedding: [0.0123456789, -0.0234567891, ...]
-- Vector of 1536 floating-point numbers
-- Each number typically ranges from -1.0 to 1.0
-- Represents semantic features of the topic
-- Generated by OpenAI's embedding model

-- embedding_model: 'text-embedding-ada-002'
-- Identifies which AI model created this embedding
-- Important for version control and model comparisons
-- Allows upgrading to newer models without losing old data

-- source_text: 'Two Pointers: A programming technique...'
-- The exact text that was fed to the embedding model
-- Critical for debugging and understanding similarity results
-- Should include: topic name, description, use cases, prerequisites, related concepts

-- created_at: '2024-01-15 14:30:22.123456'
-- Timestamp when the embedding was generated
-- Useful for tracking when embeddings need refresh

-- =============================================
-- QUERYING EMBEDDINGS IN PRACTICE
-- =============================================

-- Get the embedding for a specific topic
SELECT 
    t.name,
    te.embedding_model,
    array_length(te.embedding, 1) as dimension_count,
    substring(te.source_text, 1, 100) || '...' as source_preview,
    te.created_at
FROM topics t
JOIN topic_embeddings te ON t.id = te.topic_id
WHERE t.name = 'Two Pointers';

-- Find topics most similar to "Two Pointers"
SELECT 
    t.name,
    t.difficulty_level,
    ROUND((1 - (base.embedding <=> te.embedding))::numeric, 4) as similarity_score
FROM topic_embeddings base
CROSS JOIN topic_embeddings te
JOIN topics t ON te.topic_id = t.id
JOIN topics base_topic ON base.topic_id = base_topic.id
WHERE base_topic.name = 'Two Pointers'
  AND t.name != 'Two Pointers'
ORDER BY similarity_score DESC
LIMIT 5;

-- =============================================
-- STORAGE CONSIDERATIONS
-- =============================================

/*
Storage per embedding:
- vector(1536): 1536 × 4 bytes = 6,144 bytes ≈ 6KB per embedding
- Other fields: ~500 bytes (text, timestamps, etc.)
- Total per record: ~6.5KB

For 1,000 topics: ~6.5MB
For 10,000 topics: ~65MB  
For 100,000 topics: ~650MB

This is quite manageable for most applications, but you should consider:
1. Regular cleanup of old embeddings when models change
2. Compression for the source_text field if it gets very long
3. Archiving embeddings from deprecated models
*/